{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ZeroShotLearning.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1vMRhaM9gtsg_scXhTTJmiyvPKsAltNBz","authorship_tag":"ABX9TyM5bIfPokJksCKyVlBYjrRF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4cTVAgHFxAr","executionInfo":{"status":"ok","timestamp":1628518017978,"user_tz":-330,"elapsed":395,"user":{"displayName":"nidhir bhavsar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg95LpuDvkthoTWu-5D41T2kGPI0blRTbk_n2yL=s64","userId":"11097163690288156725"}},"outputId":"782637be-3f49-41e8-9bd4-fde6b2829428"},"source":["%cd drive/MyDrive/Biocreative/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Biocreative\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hz2uLs7CFup9","executionInfo":{"status":"ok","timestamp":1628518019276,"user_tz":-330,"elapsed":627,"user":{"displayName":"nidhir bhavsar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg95LpuDvkthoTWu-5D41T2kGPI0blRTbk_n2yL=s64","userId":"11097163690288156725"}},"outputId":"e5ec6082-118b-4584-f071-be5ba1468fc8"},"source":["!nvidia-smi"],"execution_count":4,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mFyIoBDYyZ6I"},"source":["### main model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tO1dirHYrbtZ","executionInfo":{"status":"ok","timestamp":1628518026092,"user_tz":-330,"elapsed":6825,"user":{"displayName":"nidhir bhavsar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg95LpuDvkthoTWu-5D41T2kGPI0blRTbk_n2yL=s64","userId":"11097163690288156725"}},"outputId":"b45bc89a-5ee8-424a-8714-aa815e0c38e7"},"source":["!pip install transformers"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 7.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 39.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 40.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 40.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xZlkeC1rhKh","outputId":"3190f85b-6606-4075-a29f-06b636906358"},"source":["!pip install pytorch-lightning"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-1.4.1-py3-none-any.whl (915 kB)\n","\u001b[K     |████████████████████████████████| 915 kB 8.6 MB/s \n","\u001b[?25hCollecting tensorboard!=2.5.0,>=2.2.0\n","  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 28.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.41.1)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (5.4.1)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n","Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n","\u001b[K     |████████████████████████████████| 118 kB 62.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.7.4.3)\n","Collecting pyDeprecate==0.3.1\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Collecting torchmetrics>=0.4.0\n","  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n","\u001b[K     |████████████████████████████████| 234 kB 53.1 MB/s \n","\u001b[?25hCollecting future>=0.17.1\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 38.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu102)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 42.4 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.36.2)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.34.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.17.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.32.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.3.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.12.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.8.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (57.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.6.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.1.1)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n","\u001b[K     |████████████████████████████████| 294 kB 50.0 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n","Collecting async-timeout<4.0,>=3.0\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n","\u001b[K     |████████████████████████████████| 142 kB 61.4 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.5.0)\n","Building wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=0f0c8d5eb83664b6ab09da032736d033e6fc90957b10916216b54d166ecb861d\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","Successfully built future\n","Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, torchmetrics, tensorboard, pyDeprecate, future, pytorch-lightning\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.7.0 future-0.18.2 multidict-5.1.0 pyDeprecate-0.3.1 pytorch-lightning-1.4.1 tensorboard-2.6.0 torchmetrics-0.4.1 yarl-1.6.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pqmj8seDi-IE"},"source":["!pip install torchmetrics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9FyLzJgyrtvs"},"source":["import pandas as pd\n","import numpy as np\n","\n","import random\n","from collections import defaultdict\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.metrics.functional import accuracy, f1\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","from pytorch_lightning.loggers import TensorBoardLogger\n","\n","from torchmetrics.functional.classification.auroc import auroc\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, multilabel_confusion_matrix\n","\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","\n","%matplotlib inline  \n","%config InlineBackend.figure_format='retina'\n","\n","RANDOM_SEED = 42\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","rcParams['figure.figsize'] = 12, 8\n","\n","pl.seed_everything(RANDOM_SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQ_xeGhnsltP"},"source":["df = pd.read_csv('Biocreative/preprocessed.csv')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dYfhLQO3tCNY"},"source":["train_df, val_df = train_test_split(df, test_size=0.1)\n","train_df.shape, val_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jixAuZmOtJPb"},"source":["LABEL_COLUMNS = df.columns.tolist()[1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5VFzHheatMWC"},"source":["BERT_MODEL_NAME = 'dmis-lab/biobert-base-cased-v1.1'\n","tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGkoxcA8tY-e"},"source":["----\n","### testing current approach \n","----\n","*Note: below code is processed on single example*\n","- tokenization\n","- concationation of sequence and label encodings\n","- pass through model\n","- compute loss *(BCELoss)*"]},{"cell_type":"code","metadata":{"id":"FpyVRcxrtVVy"},"source":["sample_item = train_df.abstract[0]\n","print(sample_item)\n","encoding = tokenizer.encode_plus(\n","    sample_item,\n","    add_special_tokens=True,\n","    max_length=511,\n","    return_token_type_ids=False,\n","    padding=\"max_length\",\n","    truncation=True,\n","    return_attention_mask=True,\n","    return_tensors='pt',\n",")\n","encoding.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3C7rmhHuXFN"},"source":["encoding[\"input_ids\"].shape, encoding[\"attention_mask\"].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4e9-W8IK7sXU"},"source":["# encoding labels\n","sample = train_df.iloc[0]\n","label_dict = sample[1:].to_dict()\n","true_labels = [key for key, value in label_dict.items() if value == 1]\n","print(true_labels)\n","# see something for multiple labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lfb3ordBLzTd"},"source":["label_encoding = tokenizer.encode_plus(\n","    true_labels[0],\n","    add_special_tokens=False,\n","    return_tensors=\"pt\",\n","    return_token_type_ids=False\n",")\n","label_encoding.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UhGKC_h8MRsC"},"source":["label_encoding[\"input_ids\"], tokenizer.convert_ids_to_tokens(label_encoding[\"input_ids\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4XPf0iBfMVTg"},"source":["abstract_input = encoding[\"input_ids\"]\n","label_input = label_encoding[\"input_ids\"]\n","input_tensor = torch.cat((abstract_input, label_input), 1)\n","print(f\"abstract input encoding shape {abstract_input.shape}\")\n","print(f\"label input encoding shape {label_input.shape}\")\n","print(f\"resultant input encoding shape {input_tensor.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FRtSw25OtwO"},"source":["# same goes with attention mask\n","attention_tensor = torch.cat((encoding[\"attention_mask\"], label_encoding[\"attention_mask\"]), 1)\n","print(attention_tensor.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dlNPRQVLMwiO"},"source":["# initialize bert model\n","bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8e5pI_WjPMgt"},"source":["output = bert_model(input_tensor, attention_tensor)\n","print(output.last_hidden_state.shape, output.pooler_output.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k1SjTCGHPTqD"},"source":["# fully connected layer\n","fc_layer = nn.Linear(bert_model.config.hidden_size, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_51UwwjmPTmf"},"source":["Output = fc_layer(output.pooler_output)\n","Output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_jHhcQcPTkm"},"source":["# define criterion\n","criterion = nn.BCELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MzYW9xIIT3ZO"},"source":["truth_value = torch.Tensor([1])\n","truth_value.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ceVFVpJaUDj4"},"source":["torch.sigmoid(Output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0maL6LSaWPzd"},"source":["truth_value = torch.Tensor([1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pta3LMDTbkP"},"source":["loss = criterion(torch.sigmoid(Output), truth_value.view(1, 1))\n","loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bkIGUzw31mi-"},"source":["----\n","### uncomment below cells to create ZSL data\n","----\n","- *Note*: data has already been created @ /Biocreative/ZSL.csv"]},{"cell_type":"code","metadata":{"id":"7j89zk4t0s7k"},"source":["# Dataset Format \n","# -----------------------------------\n","# |Sequence 1 | Annotation1 | Label |\n","# -----------------------------------\n","# |Sequence 2 | Annotation2 | Label |\n","# -----------------------------------"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BdJL9DVJ1xkF"},"source":["# def positive_data(example):\n","\n","#   sequence = [example[\"abstract\"]]\n","#   label_dict = example[1:].to_dict()\n","#   labels = [key for key, value in label_dict.items() if value == 1]\n","#   return zip(sequence * len(labels), labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NwTG9dD33sl_"},"source":["# def negative_data(example):\n","\n","#   sequence = [example[\"abstract\"]]\n","#   label_dict = example[1:].to_dict()\n","#   labels = [key for key, value in label_dict.items() if value != 1]\n","#   label = labels[random.randint(0, len(labels)-1)]\n","#   return zip(sequence, [label])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uIN4hOwu3saM"},"source":["# # conversion sequece\n","# train_dict = defaultdict(list)\n","\n","# for idx, row in df.iterrows():\n","#   zipped_object = positive_data(row)\n","#   for key, value in zipped_object:\n","#     train_dict[\"Sequence\"].append(key)\n","#     train_dict[\"Label\"].append(value)\n","#     train_dict[\"Truth_Value\"].append(True)\n","  \n","#   zipped_object = negative_data(row)\n","#   for key, value in zipped_object:\n","#     train_dict[\"Sequence\"].append(key)\n","#     train_dict[\"Label\"].append(value)\n","#     train_dict[\"Truth_Value\"].append(False)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TSvUj4d9QbO"},"source":["# ZSL_df = pd.DataFrame.from_dict(train_dict, orient='index').T\n","# ZSL_df = ZSL_df.sample(frac=1).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TS2htc8xECRE"},"source":["# ZSL_df.to_csv('drive/MyDrive/Biocreative/Biocreative/ZSL.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ZmN0QNYSyDF"},"source":["# data = pd.read_csv('Biocreative/ZSL.csv')\n","# data.rename(columns={\n","#     \"Label\": \"Annotation\",\n","#     \"Truth_Value\": \"Label\"\n","# }, inplace=True, errors='raise')\n","# data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t9PfFw1cTBER"},"source":["# data.to_csv('Biocreative/ZSL.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ggrwo9RpUBjT"},"source":["# convert true/fa;se to 0/1\n","# data = pd.read_csv('Biocreative/ZSL.csv')\n","# data.Label = data.Label.astype(bool)\n","# data.head()\n","# data.to_csv('Biocreative/ZSL.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S1WOa6T21o1D"},"source":["### continuation"]},{"cell_type":"code","metadata":{"id":"XTiqDsu7F-7p"},"source":["#load ZSL data and start creating a model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cWRwVm5OJp2y"},"source":["data = pd.read_csv('Biocreative/ZSL.csv')\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ml6mtoSQLMyQ"},"source":["train_df, val_df = train_test_split(data, test_size=0.05)\n","train_df.shape, val_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bNm-WISbPJ6J"},"source":["## data analysis\n"]},{"cell_type":"code","metadata":{"id":"HQxnwXVqLNrT"},"source":["pd.Series([(data.Label).sum(), (~data.Label).sum()], [\"positive\", \"negative\"]).sort_values().plot(kind=\"barh\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NceS5eNLO7nK"},"source":["token_counts = []\n","\n","for _, row in data.iterrows():\n","  token_count = len(tokenizer.encode(\n","      row[\"Sequence\"],\n","      max_length=512,\n","      truncation=True\n","  ))\n","\n","  token_counts.append(token_count)\n","\n","sns.histplot(token_counts)\n","plt.xlim([0, 512])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4Q7psWvO7jF"},"source":["# key observation: around 875 sentence are getting truncated and thus model will ignore the end context of those sequences."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbCCSOeuRMyw"},"source":["----\n","## model classes \n","----\n","- TopicAnnotationDataset\n","- TopicAnnotationDataLoader\n","- TopicAnnotationTagger"]},{"cell_type":"code","metadata":{"id":"_MX9jxd4vOfM"},"source":["# TopicAnnotationDataset\n","# TopicAnnotationDataLoader\n","# TopicAnnotationTagger"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTZasKPMhgKp"},"source":["MAX_TOKEN_COUNT = 512"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rM2LblO0Ex0"},"source":["class TopicAnnotationDataset(Dataset):\n","\n","  def __init__(\n","    self,\n","    data: pd.DataFrame,\n","    tokenizer: AutoTokenizer,\n","    max_token_len: int = 128\n","  ):\n","    self.tokenizer = tokenizer\n","    self.data = data\n","    self.max_token_len = max_token_len\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def concat_tensors(self, tensor1, tensor2):\n","    return torch.cat((tensor1, tensor2), 1)\n","\n","  def __getitem__(self, index=int):\n","    \n","    data_row = self.data.iloc[index]\n","\n","    sequence = data_row.Sequence\n","    annotation = data_row.Annotation\n","    label = data_row.Label.astype(int)\n","\n","    sequence_encoding = self.tokenizer.encode_plus(\n","        sequence,\n","        add_special_tokens=True,\n","        max_length=self.max_token_len-3,\n","        return_token_type_ids=False,\n","        padding=\"max_length\",\n","        truncation=True,\n","        return_attention_mask=True,\n","        return_tensors='pt',\n","    )\n","\n","    label_encoding = self.tokenizer.encode_plus(\n","        annotation,\n","        max_length=3,\n","        padding=\"max_length\",\n","        add_special_tokens=False,\n","        return_token_type_ids=False,\n","        return_attention_mask=True,\n","        return_tensors=\"pt\",\n","    )\n","\n","    encoding = {\n","        \"input_ids\": self.concat_tensors(sequence_encoding[\"input_ids\"], label_encoding[\"input_ids\"]),\n","        \"attention_mask\": self.concat_tensors(sequence_encoding[\"attention_mask\"], label_encoding[\"attention_mask\"])\n","    }\n","\n","    return dict(\n","        text=f\"{sequence} {annotation}\",\n","        input_ids=encoding[\"input_ids\"].flatten(),\n","        attention_mask=encoding[\"attention_mask\"].flatten(),\n","        label=torch.FloatTensor([label]) \n","    )  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZeeFIQ_af94t"},"source":["train_dataset = TopicAnnotationDataset(\n","    train_df,\n","    tokenizer,\n","    max_token_len=MAX_TOKEN_COUNT\n",")\n","sample_item = train_dataset[0]\n","sample_item.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TjQ054fix0qV"},"source":["sample_item[\"input_ids\"].shape, sample_item[\"attention_mask\"].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-FktXbImqb4B"},"source":["----\n","### implementing a sample batch on bert model\n","----"]},{"cell_type":"code","metadata":{"id":"mbFAHV6SiOCU"},"source":["bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbTZwGbvpnQE"},"source":["sample_batch = next(iter(DataLoader(train_dataset, batch_size=8, num_workers=2)))\n","sample_batch[\"input_ids\"].shape, sample_batch[\"attention_mask\"].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1PNnStxYp7yG"},"source":["output = bert_model(sample_batch[\"input_ids\"], sample_batch[\"attention_mask\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0q50iuoqEj2"},"source":["output.last_hidden_state.shape, output.pooler_output.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBPq4qSqqVuM"},"source":["output.pooler_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MZDwp30qq9lU"},"source":["# last hidden state shape\n","bert_model.config.hidden_size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XIOuBh8xqxVJ"},"source":["### continutation"]},{"cell_type":"code","metadata":{"id":"lY76nwhGqX9H"},"source":["class TopicAnnotationDataModule(pl.LightningDataModule):\n","\n","  def __init__(self, train_df, test_df, tokenizer, batch_size=8, max_token_len=128):\n","    \n","    super().__init__()\n","    self.batch_size = batch_size\n","    self.train_df = train_df\n","    self.test_df = test_df\n","    self.tokenizer = tokenizer\n","    self.max_token_len = max_token_len\n","\n","  def setup(self, stage=None):\n","    \n","    self.train_dataset =  TopicAnnotationDataset(\n","        self.train_df,\n","        self.tokenizer,\n","        self.max_token_len\n","    )\n","\n","    self.test_dataset = TopicAnnotationDataset(\n","        self.test_df,\n","        self.tokenizer,\n","        self.max_token_len\n","    )\n","\n","  def train_dataloader(self):\n","\n","    return DataLoader(\n","      self.train_dataset,\n","      batch_size=self.batch_size,\n","      shuffle=True,\n","      num_workers=2\n","    )\n","\n","  def val_dataloader(self):\n","\n","    return DataLoader(\n","      self.test_dataset,\n","      batch_size=self.batch_size,\n","      num_workers=2\n","    )\n","\n","  def test_dataloader(self):\n","    \n","    return DataLoader(\n","      self.test_dataset,\n","      batch_size=self.batch_size,\n","      num_workers=2\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ob7N0-Xbrq1t"},"source":["N_EPOCHS = 10\n","BATCH_SIZE = 8\n","\n","data_module = TopicAnnotationDataModule(\n","    train_df,\n","    val_df,\n","    tokenizer,\n","    batch_size=BATCH_SIZE,\n","    max_token_len=MAX_TOKEN_COUNT\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2m7Ol1XrxUU"},"source":["class TopicAnnotationTagger(pl.LightningModule):\n","\n","  def __init__(\n","      self,\n","      n_classes: int,\n","      n_training_steps=None,\n","      n_warmup_steps=None\n","  ):\n","\n","    super().__init__()\n","    self.bert = AutoModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n","    self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n","    self.n_training_steps = n_training_steps\n","    self.n_warmup_steps = n_warmup_steps\n","    self.criterion = nn.BCELoss()\n","\n","  def forward(self, input_ids, attention_mask, label=None):\n","\n","    output = self.bert(input_ids, attention_mask=attention_mask)\n","    output = self.classifier(output.pooler_output)\n","    output = torch.sigmoid(output)\n","    loss = 0\n","    if label is not None:\n","      loss = self.criterion(output, label)\n","    return loss, output\n","\n","  def training_step(self, batch, batch_idx):\n","\n","    input_ids = batch[\"input_ids\"]\n","    attention_mask = batch[\"attention_mask\"]\n","    label = batch[\"label\"]\n","\n","    loss, outputs = self(input_ids, attention_mask, label)\n","    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n","    return {\"loss\": loss, \"predictions\": outputs, \"label\": label}\n","\n","  def validation_step(self, batch, batch_idx):\n","\n","    input_ids = batch[\"input_ids\"]\n","    attention_mask = batch[\"attention_mask\"]\n","    label = batch[\"label\"]\n","    loss, outputs = self(input_ids, attention_mask, label)\n","    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n","    return loss\n","\n","  def test_step(self, batch, batch_idx):\n","    \n","    input_ids = batch[\"input_ids\"]\n","    attention_mask = batch[\"attention_mask\"]\n","    label = batch[\"label\"]\n","    loss, outputs = self(input_ids, attention_mask, label)\n","    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n","    return loss\n","\n","  def training_epoch_end(self, outputs):\n","\n","    label = []\n","    predictions = []\n","    for output in outputs:\n","      for out_label in output[\"label\"].detach().cpu():\n","        label.append(out_label)\n","      for out_predictions in output[\"predictions\"].detach().cpu():\n","        predictions.append(out_predictions)\n","\n","    label = torch.stack(label).int()\n","    predictions = torch.stack(predictions)\n","\n","    roc_auc = auroc(predictions, label)\n","    self.logger.experiment.add_scalar(\"roc_auc/Train\", roc_auc, self.current_epoch)\n","  \n","  def configure_optimizers(self):\n","\n","    optimizer = AdamW(self.parameters(), lr=2e-5)\n","\n","    scheduler = get_linear_schedule_with_warmup(\n","      optimizer,\n","      num_warmup_steps=self.n_warmup_steps,\n","      num_training_steps=self.n_training_steps\n","    )\n","\n","    return dict(\n","        optimizer=optimizer,\n","      lr_scheduler=dict(\n","        scheduler=scheduler,\n","        interval='step'\n","      )\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TWaqSjQ6u6Mk"},"source":["steps_per_epoch=len(train_df) // BATCH_SIZE\n","total_training_steps = steps_per_epoch * N_EPOCHS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Ef3MkqtucWG"},"source":["warmup_steps = total_training_steps // 5\n","warmup_steps, total_training_steps"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wn9OaNLQwwuW"},"source":["model = TopicAnnotationTagger(\n","    n_classes=len(LABEL_COLUMNS),\n","    n_warmup_steps=warmup_steps,\n","    n_training_steps=total_training_steps\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5LjnVmQwwxy2"},"source":["checkpoint_callback = ModelCheckpoint(\n","    dirpath=\"ZSL_checkpoints\",\n","    filename=\"best-checkpoint\",\n","    save_top_k=1,\n","    verbose=True,\n","    monitor=\"val_loss\",\n","    mode=\"min\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XMHJDvpw6xq"},"source":["logger = TensorBoardLogger(\"ZSL_lightning_logs\", name=\"topic-annotations\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uH1waQ3bw86Y"},"source":["early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G55RhnAEw-Lp"},"source":["trainer = pl.Trainer(\n","    logger=logger,\n","    checkpoint_callback=True,\n","    callbacks=[checkpoint_callback, early_stopping_callback],\n","    max_epochs=N_EPOCHS,\n","    gpus=-1,\n","    progress_bar_refresh_rate=30\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3vSj-9ew_Te"},"source":["trainer.fit(model, data_module)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"STl1DnKy4VcC"},"source":["# train_at_epoch_end is functioning oddly so see through it by the end of the day"],"execution_count":null,"outputs":[]}]}